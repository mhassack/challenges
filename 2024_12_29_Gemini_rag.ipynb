{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOBD4/3h1Iv6INl2rAz0Ag9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mhassack/challenges/blob/master/2024_12_29_Gemini_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain_google_genai langchain_google_community langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "collapsed": true,
        "id": "2NY6ztOdxHMm",
        "outputId": "183204b8-65a5-495f-8f77-35a6764b784f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "      <style>\n",
              "        pre {\n",
              "            white-space: pre-wrap;\n",
              "        }\n",
              "      </style>\n",
              "      "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGHDg4qpvPpw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_google_community import BigQueryVectorStore\n",
        "from langchain.prompts import PromptTemplate, StringPromptTemplate\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "from langchain_core.runnables import RunnableLambda"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(level=logging.DEBUG)"
      ],
      "metadata": {
        "id": "oENTrH2RwOhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RUNNING_IN_COLAB = True\n",
        "\n",
        "if RUNNING_IN_COLAB:\n",
        "    logger.debug(\"Running in Colab\")\n",
        "    from google.colab import userdata\n",
        "    GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    from IPython.display import HTML, display\n",
        "\n",
        "    def set_css():\n",
        "      display(HTML('''\n",
        "      <style>\n",
        "        pre {\n",
        "            white-space: pre-wrap;\n",
        "        }\n",
        "      </style>\n",
        "      '''))\n",
        "    get_ipython().events.register('pre_run_cell', set_css)\n",
        "else:\n",
        "    logger.debug(\"Loading dotenv\")\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()  # take environment variables from .env.\n",
        "    GEMINI_API_KEY = os.environ['GEMINI_API_KEY']\n",
        "\n",
        "# Define the name of the GCP project\n",
        "GOOGLE_CLOUD_PROJECT = \"ai-experiments-445909\"\n",
        "\n",
        "# Define the model to use\n",
        "#GEMINI_MODEL=\"gemini-1.5-pro\"\n",
        "GEMINI_MODEL=\"gemini-2.0-flash-exp\"\n"
      ],
      "metadata": {
        "id": "LOAc9YTGwp1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create our embeddings object\n",
        "logger.debug(\"Creating the embeddings object\")\n",
        "embedding = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"models/text-embedding-004\", google_api_key=GEMINI_API_KEY\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "lRlNneHswuCG",
        "outputId": "69b59ab9-a388-4e17-9ee5-9b2b97ccae55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "      <style>\n",
              "        pre {\n",
              "            white-space: pre-wrap;\n",
              "        }\n",
              "      </style>\n",
              "      "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the BQ bits\n",
        "BIGQUERY_DATASET_ID = \"rag_experiments\"\n",
        "BIGQUERY_TABLE_ID = \"test_embeddings_overlap\"\n",
        "BIGQUERY_LOCATION = \"europe-west2\"\n",
        "\n",
        "# Create the BQ vector store\n",
        "logger.debug(\"Create the BigQuery Vector Store\")\n",
        "store = BigQueryVectorStore(\n",
        "    project_id=GOOGLE_CLOUD_PROJECT,\n",
        "    dataset_name=BIGQUERY_DATASET_ID,\n",
        "    table_name=BIGQUERY_TABLE_ID,\n",
        "    embedding=embedding,\n",
        "    location=BIGQUERY_LOCATION\n",
        ")\n",
        "\n",
        "# Retrieve and generate using the relevant snippets of the blog.\n",
        "logger.debug(\"Geetting the BQ store as a retreiver\")\n",
        "retriever = store.as_retriever(search_kwargs={\"k\": 10})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "wJ25B4tewwUD",
        "outputId": "4f045406-05f2-43f3-950e-340ea5b77984"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "      <style>\n",
              "        pre {\n",
              "            white-space: pre-wrap;\n",
              "        }\n",
              "      </style>\n",
              "      "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:langchain_google_community.bq_storage_vectorstores._base:BigQuery table ai-experiments-445909.rag_experiments.test_embeddings_overlap initialized/validated as persistent storage. Access via BigQuery console:\n",
            " https://console.cloud.google.com/bigquery?project=ai-experiments-445909&ws=!1m5!1m4!4m3!1sai-experiments-445909!2srag_experiments!3stest_embeddings_overlap\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the LLM interface\n",
        "logger.debug(\"Create the LLM object\")\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=GEMINI_MODEL,\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        "    google_api_key=GEMINI_API_KEY\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "0frYFbTpw2KF",
        "outputId": "b36dcb93-44f3-4e60-f1a1-bd0f41e208be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "      <style>\n",
              "        pre {\n",
              "            white-space: pre-wrap;\n",
              "        }\n",
              "      </style>\n",
              "      "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#question = \"In the context of Asterisk, tell me what IAX is\"\n",
        "question = \"In the context of Asterisk, what does the mailboxdetail setting do?\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "gRJRGymk5rHY",
        "outputId": "13e7f9b6-048e-4d3f-ed83-7b30d48ded1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "      <style>\n",
              "        pre {\n",
              "            white-space: pre-wrap;\n",
              "        }\n",
              "      </style>\n",
              "      "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the non RAG chain\n",
        "non_rag_chain = (\n",
        "    llm\n",
        "    | StrOutputParser())\n",
        "\n",
        "# Query the LLM using the non RAG chain\n",
        "response = non_rag_chain.invoke(question)\n",
        "print(f\"Non RAG response: {response}\")"
      ],
      "metadata": {
        "id": "y8WtYtsxyKYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inspect(state):\n",
        "    print(state)\n",
        "    return state\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# Create the prompt\n",
        "logger.debug(\"Defining the LLM prompt\")\n",
        "template_question = \"\"\"\n",
        "You are an assistant for question-answering tasks. Use the following pieces of\n",
        "retrieved context to answer the question. If you don't know the answer, just say\n",
        "that you don't know. Use three sentences maximum and keep the answer concise.\n",
        "Question: {question}\n",
        "Context: {context}\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "template_document = \"\"\"\n",
        "Please write me a short document that answers the question below.\n",
        "The document should be approximetly 2 pages of A4 long. If necessary, use the\n",
        "following pieces of retrieved context to answer the question. If you don't know\n",
        "the answer, just say that you don't know.\n",
        "Question: {question}\n",
        "Context: {context}\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "ragPrompt = PromptTemplate.from_template(template_question)\n",
        "\n",
        "# Create the RAG chain\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | ragPrompt\n",
        "    | RunnableLambda(inspect)\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Query the LLM via the RAG chain\n",
        "response = rag_chain.invoke(question)\n",
        "print(f\"\\n\\nRAG response: {response}\")\n",
        "\n",
        "logger.debug(\"DONE\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "fpd5Uul5xDcJ",
        "outputId": "d13b5ff3-083b-49a4-ea12-92e7767e72af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "      <style>\n",
              "        pre {\n",
              "            white-space: pre-wrap;\n",
              "        }\n",
              "      </style>\n",
              "      "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text=\"\\nYou are an assistant for question-answering tasks. Use the following pieces of \\nretrieved context to answer the question. If you don't know the answer, just say \\nthat you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: In the context of Asterisk, what does the mailboxdetail setting do? \\nContext: name\\nThis is the name of the mailbox owner. The company directory uses the text in this\\nfield to allow callers to spell usernames.\\nemail\\nThis is the email address of the mailbox owner. Asterisk can send voicemail noti-\\nfications (including the voicemail message itself) to the specified email box.\\n\\n[default]\\n; regular mailbox with email notification\\n101 => 4242,Example Mailbox,somebody@asteriskdocs.org\\n; more advanced mailbox with email and pager notification and a couple of\\n; special options\\n102 => 9855,Another User,another@asteriskdocs.org,pager@asteriskdocs.org,\\nattach=no|tz=central\\n\\nof a simple statement of whether new and old messages exist. mailboxdetail can\\nalso be set on a per-peer basis:\\nmailboxdetail=yes\\nmaxjitterbuffer (channel)\\nThis parameter is used to set the maximum size of the jitter buffer, in milliseconds.\\n\\npager_email\\nThis is the email address of the mailbox owner’s pager or cell phone. Asterisk can\\nsend a short voicemail notification message to the specified email address.\\noptions\\nThis field is a list of options that sets the mailbox owner’s time zone and overrides\\n\\nthe amount of transcoding that Asterisk does when the voicemail is played back.\\nserveremail\\nProvides the email address from which voicemail notifications should be sent.\\nattach\\nSpecifies whether or not Asterisk should attach the voicemail sound file to the\\nvoicemail notification email.\\nmaxmsg\\n\\nnumber. You should also specify a voicemail context.\\nThe options string can contain zero or more of the following options:\\ns\\nSkip the password check.\\np\\nThis option tells Asterisk to interpret the mailbox as a number that should be pre-\\n\\npended to the mailbox entered by the caller. This is most often used when many\\nvoicemail boxes for different companies are hosted on the same Asterisk server.\\ng(gain)\\nWhen recording voicemail messages, increase the volume by gain. This option\\nshould be specified in an integer number of decibels.\\n\\n[default] voicemail context.\\nCreating Mailboxes\\nInside each voicemail context, we define different mailboxes. The syntax for defining\\na mailbox is:\\nmailbox => password,name[,email[,pager_email[,options]]]\\nLet’s explain what each part of the mailbox definition does:\\nmailbox\\n\\nas to whether a particular voicemail message is for them personally or for a department\\nto which they belong. There is nothing unusual to configure from Asterisk’s point of\\nview; you simply call the VoiceMail() application with the desired mailbox and context,\\n\\nto vm-context if unspecified. If the options argument is set to f, Asterisk will find a\\ndirectory match based on the first name in voicemail.conf instead of the last name. If\\nthe e option is specified, Asterisk will read the extension of the directory match as well\\nas the person’s name. \\nAnswer:\\n\"\n",
            "\n",
            "\n",
            "RAG response: The `mailboxdetail` setting in Asterisk controls whether a simple statement of new and old messages exists. It can be set on a per-peer basis. It does not relate to the mailbox owner's name, email, or pager email.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}